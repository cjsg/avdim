# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

import math
import time
import numpy as np
import scipy.stats as st
from functools import partial

import torch
from torch.autograd import grad

import foolbox
from foolbox.distances import Linfinity, MSE

from data import CIFAR10, IMGNET12, MNIST


def do_pass(net, loader, args, means, stds):
    correct = total = 0.
    device = next(net.parameters()).device
    means = torch.FloatTensor(means).to(device)
    stds = torch.FloatTensor(stds).to(device)
    for i, (inputs, targets) in enumerate(loader):
        inputs, targets = inputs.to(device), targets.to(device)
        inputs = (inputs - means) / stds
        outputs = net(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += targets.size(0)
        correct += predicted.eq(targets.data).float().sum().item()

        if args.log_step is not None and i % args.log_step == 0:
            print("Batch: %03d Acc: %4.1f" % (i, 100 * correct / total))

    return 100 * correct / total


def classify(net, x, args, means, stds):
    device = next(net.parameters()).device
    x = x.to(device).view(1, 3, args.img_size, args.img_size)
    means = torch.FloatTensor(means).to(device)
    stds = torch.FloatTensor(stds).to(device)
    x = ((x - means) / stds).detach()
    x.requires_grad = True
    y = net(x)
    g = grad(y.sum(), x)[0].view(x.size(0), -1).norm().item()
    _, top_indices = y.data.cpu().view(-1).topk(2)
    return top_indices[0].item(), g


def myPrint(string, args):
    if args.log_vul:
        print(string)


def conf95(a):
    return st.t.interval(
        0.95, len(a) - 1, loc=np.nanmean(a),
        scale=st.sem(a, nan_policy='omit'))


def compute_vulnerability(args, attack_name, net, n_attacks=-1):
    """
    Computes vulnerability using foolbox package of net

    Parameters
    ----------
    args : :class:`argparse.ArgumentParser`
        The arguments passed to main.py
    attack_name : string
        The attack type. Must be one of
        {'l1', 'l2', 'itl1', 'itl2', 'pgd', 'deepfool'}
    net : :class:`torch.nn.Module`
        The network whose vulnerability is computed.
    n_attacks : int
        The number of attacks to use for the computation of vulnerbaility.
        If -1 or greater than dataset-size, uses the entire dataset.
        Default: -1.

    """

    print('\nStarting attacks of type ' + attack_name)

    # Reload data without normalizing it
    print('> Loading dataset %s...' % args.dataset)
    if args.dataset == 'mnist':
        _, loader = MNIST(
            root=args.datapath, bs=args.bs, valid_size=0.,
            size=args.img_size, normalize=False)
    elif args.dataset == 'cifar':
        _, loader = CIFAR10(
            root=args.datapath, bs=args.bs, valid_size=0.,
            size=args.img_size, normalize=False)
    elif args.dataset == 'imgnet12':
        _, loader = IMGNET12(
            root=args.datapath, bs=args.bs, valid_size=0.,
            size=args.img_size, normalize=False)
    else:
        raise NotImplementedError
    print('> Done.')

    # Image-normalizations (must be same as in data.py)
    if args.raw_inputs:
        means = [0., 0., 0.]
        stds = [1., 1., 1.]
    elif args.dataset == "mnist":
        means = [0.1307]
        stds = [0.3081]
    elif args.dataset == "cifar":
        means = [0.4914, 0.4822, 0.4465]
        stds = [0.2023, 0.1994, 0.2010]
    elif args.dataset == "imgnet12":
        means = [.453, .443, .403]
        stds = {
            256: [.232, .226, .225],
            128: [.225, .218, .218],
            64: [.218, .211, .211],
            32: [.206, .200, .200]
        }[args.img_size]
    else:
        raise NotImplementedError

    means = np.array(means).reshape(-1, 1, 1)
    stds = np.array(stds).reshape(-1, 1, 1)

    net.eval()
    print('> Computing test accuracy...')
    te_acc = do_pass(net, loader, args, means, stds)
    print('> Done. Computed test accuracy: %5.2f' % te_acc)

    # construct attack
    bounds = (0, 1)
    model = foolbox.models.PyTorchModel(net, bounds=bounds,
                                        preprocessing=(means, stds),
                                        num_classes=args.categories)

    # Choosing attack type
    if attack_name == 'l1':
        # vulnerability increases like sqrt(d) \propto img_size
        # therefore, we divide the linfty-threshold by img_size
        attack = partial(foolbox.attacks.FGSM(model, distance=Linfinity),
                         epsilons=1000, max_epsilon=1. / args.img_size)
    elif attack_name == 'l2':
        # to be visually constant, the l2-threshold increases like sqrt d;
        # but vulnerability also increases like sqrt d;
        # therefore, use constant max_epsilon accross dimension d
        attack = partial(foolbox.attacks.GradientAttack(model, distance=MSE),
                         epsilons=1000, max_epsilon=1.)
    elif attack_name == 'itl1':
        it, eps = 10, 1. / args.img_size
        attack = partial(
            foolbox.attacks.LinfinityBasicIterativeAttack(
                model, distance=Linfinity),
            iterations=it, epsilon=eps,
            stepsize=eps / (.75 * it), binary_search=True)
    elif attack_name == 'itl2':
        it, eps = 10, 1.
        attack = partial(
            foolbox.attacks.L2BasicIterativeAttack(
                model, distance=MSE),
            iterations=it, epsilon=eps,
            stepsize=eps / (.75 * it), binary_search=True)
    elif attack_name == 'pgd':
        it, eps = 10, 1. / args.img_size
        attack = partial(foolbox.attacks.RandomPGD(model, distance=Linfinity),
                         iterations=it, epsilon=eps,
                         stepsize=eps / (.75 * it), binary_search=True)
    elif attack_name == 'deepFool':
        attack = foolbox.attacks.DeepFoolAttack(model, distance=MSE)
    elif attack_name == 'boundary':
        attack = partial(foolbox.attacks.BoundaryAttack(model, distance=MSE),
                         iterations=2000, log_every_n_steps=np.Infinity,
                         verbose=False)
    else:
        raise NotImplementedError(
            "attack_name must be 'l1', 'l2', 'itl1', 'itl2', "
            "'deepFool' or 'boundary'")

    n_iterations = 0
    results = {}
    results['l2_snr'] = []
    results['clean_grad'] = []
    results['dirty_grad'] = []
    results['l2_norm'] = []
    results['linf_norm'] = []
    n_fooled = 0

    print('> Creating empty image-tensors')
    n_saved = 64 if (n_attacks == -1) else min(n_attacks, 64)
    clean_images = torch.zeros(n_saved, 3, args.img_size, args.img_size)
    dirty_images = torch.zeros(n_saved, 3, args.img_size, args.img_size)
    print('> Done.')

    myPrint(("{:>15} " * 5).format(
        "clean_grad", "dirty_grad", "linf_norm", "l2_norm", "l2_snr"), args)

    t0 = time.time()
    for i, (images, labels) in enumerate(loader):

        if n_iterations == n_attacks:
            break

        for i, clean_image in enumerate(images):
            clean_label, clean_grad = classify(net, clean_image,
                                               args, means, stds)
            dirty_image_np = attack(clean_image.numpy(), clean_label)

            if dirty_image_np is not None:  # i.e. if adversarial was found
                dirty_image = torch.Tensor(dirty_image_np)
                _, dirty_grad = classify(net, dirty_image,
                                         args, means, stds)

                if i < n_saved:  # only save n_saved first images
                    dirty_images[i] = dirty_image.clone()
                    clean_images[i] = clean_image.clone()

                l2_norm = (clean_image - dirty_image).norm().item()
                linf_norm = (clean_image - dirty_image).abs().max().item()
                l2_snr = 20. * math.log10(
                    clean_image.norm().item() / (l2_norm + 1e-6))
            else:
                l2_snr = dirty_grad = l2_norm = linf_norm = np.NaN

            results['l2_snr'].append(l2_snr)
            results['clean_grad'].append(clean_grad)

            results['dirty_grad'].append(dirty_grad)
            results['l2_norm'].append(l2_norm)
            results['linf_norm'].append(linf_norm)

            fmt_str = "{:>15.6f} " * 5
            if ((attack.func._default_distance == MSE and
                    l2_norm < .005 * np.sqrt(args.img_size)) or
                (attack.func._default_distance == Linfinity and
                    linf_norm < .005)):
                fmt_str += " * fooled!"
                n_fooled += 1
            myPrint(fmt_str.format(clean_grad, dirty_grad, linf_norm,
                                   l2_norm, l2_snr),
                    args)
            n_iterations += 1
            if n_iterations == n_attacks:
                break

    # Printing summary
    summary = {}
    print("\n Summary for network in '{}' of test accuracy {}".format(
        args.path, te_acc))
    for key, value in results.items():
        low95, high95 = conf95(value)
        print("{:>10} mean:{:>10.5f} std:{:>10.5f} conf95:({:>10.5f}, "
              "{:>10.5f}) minmax:({:>10.5f}, {:>10.5f})".format(
                  key, np.nanmean(value), np.nanstd(value), low95, high95,
                  np.nanmin(value), np.nanmax(value)))
        summary[key] = [np.nanmean(value), np.nanstd(value), low95, high95]
        percent = 100 * n_fooled / float(n_iterations)
    print("{:>10} {:10d}s".format("Time", int(time.time() - t0)))
    print("{:>10} {:10.1f}%".format("percent", percent))

    # Preparing the output
    output = dict()
    output['summary'] = summary
    output['results'] = results
    output['clean_images'] = clean_images
    output['dirty_images'] = dirty_images
    output['percent'] = percent
    output['te_acc'] = te_acc

    return output
